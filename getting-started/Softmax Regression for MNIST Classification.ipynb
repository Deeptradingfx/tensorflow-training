{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression (Multinomial Logistic Regression)\n",
    "\n",
    "It is the convention in programming that the first thing you do is print \"Hello World\".\n",
    "So, like programming has Hello World, machine learning has MNIST.\n",
    "\n",
    "### What is MNIST?\n",
    "MNIST is a standard dataset used for computer vision. It consists of images of handwritten digits like the following:\n",
    "\n",
    "![Figure 1. Sample images from MNIST dataset (from [TensorFlow](https://www.tensorflow.org/get_started/mnist/beginners).](figures/MNIST.png)\n",
    "Along with the images are the labels for each of them, indicating which digit it is. For instance, the labels for the above images are 5, 0, 4, and 1.\n",
    "\n",
    "For this tutorial, we are going to implement a simple model and train it to look at images, then predict what their labels are. However, take note that the model to be implemented here will not achieve state-of-the-art performance. We'll get to that later, at the next tutorial. For now, we shall be starting with a quite simple model called the ***Softmax Regression***.\n",
    "\n",
    "We shall accomplish the following in this tutorial:\n",
    "* Learn about the MNIST data and softmax regression\n",
    "* Implement a model for recognizing MNIST digits, based on looking at every pixel of each image.\n",
    "* Use TensorFlow to train the model to recognize the handwritten digits by having it \"look\" at thousands of examples\n",
    "* Check the model's accuracy with the test data\n",
    "\n",
    "### The MNIST Data\n",
    "The MNIST data is hosted on [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). We shall be loading the dataset using the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/darth/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/home/darth/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into two parts:\n",
    "\n",
    "|Filename|Category|File Size|\n",
    "|--------|--------|---------|\n",
    "|[train-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz)|training set images|9912422 bytes|\n",
    "|[train-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz)|training set labels|28881 bytes|\n",
    "|[t10k-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz)|test set images|1648877 bytes|\n",
    "|[t10k-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz)|test set labels|4542 bytes|\n",
    "\n",
    "The splitting of data is quite important for it is essential in machin learning to have a separate data. This way, we can determine if the model actually generalizes, and not just memorized the data.\n",
    "\n",
    "As it was mentioned a while ago, the dataset has two parts: (1) image of handwritten digit -- we'll call `x`, and (2) a corresponding label -- we'll call `y`. Both the training set and test set contain images and their corresponding labels, e.g. `x = mnist.train.images`, and `y = mnist.train.labels`.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels, and they can be interpreted as a big array of numbers:\n",
    "\n",
    "![](figures/MNIST-Matrix.png)\n",
    "\n",
    "The said array can then be flatten into a vector of 28x28 = 784 numbers. From this perspective, the MNIST images are just points in a 784-dimensional vector space.\n",
    "\n",
    "Hence, the result of a flattened `mnist.train.images` is a tensor (n-dimensional array) with a shape of `[55000, 784]`. The first dimension refers to the index of images in the dataset, while the second dimension refers to the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "![](figures/mnist-train-xs.png)\n",
    "\n",
    "As mentioned before, each image in MNIST has a corresponding label, a number between 0 and 9, representing the digit written in the image.\n",
    "\n",
    "For this tutorial, the labels will be _one-hot vectors_, i.e. a vector with 1 in a single dimension (index of the label for the image) and 0 in the rest. For instance, a label 3 would be `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`. Consequently, `mnist.train.labels` is a `[55000, 10]` array of floats.\n",
    "\n",
    "![](figures/mnist-train-ys.png)\n",
    "\n",
    "Now that you have been familiarized with the MNIST dataset, we can now write the model.\n",
    "\n",
    "### Softmax Regression (Multinomial Logistic Regression)\n",
    "\n",
    "What is **regression**? It is an approximation technique used to find or to estimate the relationships between or among variables. In other words, to determine the mapping of the input and output: \\\\(f(x) = y\\\\).\n",
    "\n",
    "There are of regression techniques available, but for this tutorial, we are going to focus on _multinomial logistic regression_ or more commonly known as _softmax regression_.\n",
    "\n",
    "Since it has already been understood that every image in MNIST is of a handwritten digit between 0 and 9, there are only 10 possible numbers a given image can be. Through softmax regression, we can look at an image and provide probabilities for each digit. For instance, the model might look at a picture of number 9, and give a probability that says it's 90% sure it's a 9, but give 5% chance to it being an 8, and scattered probabilities among others because it isn't 100% sure.\n",
    "\n",
    "Hence, if the problem domain is to determine the probabilities of an object belonging to one of several different things (classes), softmax is the one to choose. This is so because it gives a list of values between 0 and 1, that add up to 1 (a whole probability). Even when you look at other models, the most common final step is a softmax layer.\n",
    "\n",
    "A softmax regression has two steps:\n",
    "* Add up the evidence of the input being in a certain class\n",
    "* Convert the evidence to probabilities\n",
    "\n",
    "To add the evidences that a given image belongs to a particular class, we perform weighted sum of pixel intensities (the input variable _x_).\n",
    "\n",
    "![](figures/softmax-regression-scalargraph.png)\n",
    "\n",
    "<br><br>\n",
    "Converting the diagram to a set of equations,\n",
    "\n",
    "![](figures/softmax-regression-scalarequation.png)\n",
    "\n",
    "In form of matrix multiplication and vector addition, the above equations will become the following:\n",
    "\n",
    "![](figures/softmax-regression-vectorequation.png)\n",
    "\n",
    "Or in a more compact form,\n",
    "\n",
    "\\\\[y = softmax(Wx+b) \\\\]\n",
    "\n",
    "Formalizing,\n",
    "\n",
    "\\\\[evidence_{i} = \\sum_{j} W_{i,j}x_{j} + b_{i}\\\\]\n",
    "\n",
    "where \\\\(W_{i}\\\\) is the weights, \\\\(b_{i}\\\\) is the bias for class \\\\(i\\\\), and \\\\(j\\\\) is an index for summing over the pixels in the input image \\\\(x\\\\). The evidence tallies shall then be converted to predicted probabilities \\\\(y\\\\) using the \"softmax\" function:\n",
    "\n",
    "\\\\[y = softmax(evidence)\\\\]\n",
    "\n",
    "Expanding the equation,\n",
    "\n",
    "\\\\[softmax(x)_{i} = \\dfrac{exp(x_{i})}{\\sum_{j}exp(x_{j})   }\\\\]\n",
    "\n",
    "With the above equation, no hypothesis will ever get zero nor a negative weight. Softmax normalizes the weights, so that they add up to 1, forming a valid probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Regression\n",
    "\n",
    "The machine intelligence library to be used for implementing the regression model will be TensorFlow, and to use it, we must import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, we describe a graph of operations, operations such as matrix multiplication, and vector addition among others. Such operations are described using symbolic variables like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in conventional programming, `x` is not a specific value, it's a `placeholder`. An object to hold the value we shall input when we ask TensorFlow to run a computation. For the MNIST case, we want to be able to input any number of images, each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating-point numbers, with a shape of `[None, 784]`. `None` was used to denote that the dimension can be of variable length.\n",
    "\n",
    "We will also need the weights and biases for our model. The weights and biases are the parameters that the model learns through training iterations. Instead of using `placeholder`, we use `Variable` for the parameters so that it is modifiable throughout its lifetime in a TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(initial_value=tf.zeros([784, 10]))\n",
    "biases = tf.Variable(initial_value=tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Variable`s are created by initializing them with zeros since they are going to be learned. Hence, it is not of concern what they initially are.\n",
    "\n",
    "Take note that `weights` has a shape of `[784, 10]`. This is because we want to multiply the 784-dimensional image vectors by it to produce 10-dimensional vectors, consisting of evidence for different classes. Meanwhile, `biases` has a shape of `[10]` so we can add it to the output of the matrix multiplication, i.e. \\\\((Wx)\\\\) to be \\\\((Wx) + b\\\\).\n",
    "\n",
    "To implement the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = tf.nn.softmax(tf.matmul(x, weights) + biases)\n",
    "y_ = tf.matmul(x, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "To train our model, we must determine how to say if the model is good. Though in ML, we actually measure how bad a model is. That measure is called _loss_ or _cost_. It indicates how far off the predicted output is from the actual output. During the training, we try to minimize that error. The smaller the error, of course, the better.\n",
    "\n",
    "The conventional function used for computing loss is the _cross entropy_ (from information theory):\n",
    "\n",
    "\\\\[H_{y'}(y) = - \\sum_{i} y'_{i} log(y_{i})\\\\]\n",
    "\n",
    "where $y'$ is the true output, and $y$ is the predicted output.\n",
    "\n",
    "To implement cross-entropy, we must first define a new placeholder to input the correct answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can implement the cross-entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross entropy function based on formula\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# we don't use the above code since that is numerically unstable, instead we use\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the optimization algorithm of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we use SGD to minimize the loss of our model, with a learning rate of `0.5`.\n",
    "\n",
    "Let us now launch the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a TensorFlow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step loss : 2.3025848865509033\n",
      "Step loss : 0.4590816795825958\n",
      "Step loss : 0.2645539343357086\n",
      "Step loss : 0.41398847103118896\n",
      "Step loss : 0.3004906177520752\n",
      "Step loss : 0.1506330370903015\n",
      "Step loss : 0.2550148069858551\n",
      "Step loss : 0.3431825637817383\n",
      "Step loss : 0.315665602684021\n",
      "Step loss : 0.28851184248924255\n"
     ]
    }
   ],
   "source": [
    "# train 20000 times\n",
    "for step in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "    if step % 100 == 0:\n",
    "        print('Step loss : {}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Model\n",
    "\n",
    "Since we are dealing with one-hot vectors, we must determine which index is of the highest entry in a tensor along an axis. We can accomplish this using `tf.argmax()`, then we check if the labels (predicted and actual) are a match using `tf.equal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the preceding code gives us a list of booleans,\n",
    "# we should cast them to floating-point numbers,\n",
    "# then get its mean.\n",
    "# For example [True, False, True, True] would become\n",
    "# [1, 0, 1, 1] which would be 0.75\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9164\n"
     ]
    }
   ],
   "source": [
    "# finally, compute the accuracy on test data\n",
    "feed_dict={x: mnist.test.images, y: mnist.test.labels}\n",
    "print(sess.run(accuracy, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of $\\approx$92% for the simple Softmax Regression model. Is that good? Nope! It can be as high as 98-99% with Convolutional Neural Networks! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
