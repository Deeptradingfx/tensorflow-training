{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "Before going straight to a multilayer perceptron (MLP), it is important to first know _what is a perceptron_?\n",
    "\n",
    "The perceptron is the simplest form of a neural network. It consists of a single neuron with adjustable (learnable) weights, and a hard-limit function (used for decision making for its output). A single-layer two-input perceptron is shown in the following figure:\n",
    "\n",
    "![](../figures/perceptron.png)\n",
    "Image from Michael Negnevitsky's [_Artificial Intelligence: A Guide to Intelligent Systems_ (2005)](https://books.google.com.ph/books/about/Artificial_Intelligence.html).\n",
    "\n",
    "Since the perceptron has already been described, it stands to reason that MLP is just a collection of multiple instances of a perceptron. An MLP can be viewed as a simple logistic regression classifier where the inputs are transformed using a learnt non-linear transformation. It can be visualized as follows:\n",
    "\n",
    "![](../figures/mlp.png)\n",
    "\n",
    "Formally speaking, a one-layer MLP is a function \\\\(f: R^{N} \\rightarrow R^{M}\\\\), where \\\\(N\\\\) is the size of input vector \\\\(x\\\\), and \\\\(M\\\\) is the size of the output vector \\\\(f(x)\\\\), such that in matrix notation:\n",
    "\n",
    "\\\\[f(x) = G(b^{(2)} + W^{(2)} (s(b^{(1)} + W^{(1)})))\\\\]\n",
    "\n",
    "where \\\\(b^{(1)}\\\\) and \\\\(b^{(2)}\\\\) are bias vectors; \\\\(W^{(1)}\\\\) and \\\\(W^{(2)}\\\\) are weight matrices; and \\\\(G\\\\) and \\\\(s\\\\) are activation functions.\n",
    "\n",
    "The output of a this neural network is then passed on to a softmax activation function to deliver the probability distribution among classes.\n",
    "\n",
    "\\\\[y = softmax(f(x))\\\\]\n",
    "\n",
    "For this session, we are going to build an MLP for MNIST classification with the following architecture:\n",
    "\n",
    "1. Input layer\n",
    "2. Hidden layer 1 (500 neurons)\n",
    "3. Hidden layer 2 (500 neurons)\n",
    "4. Hidden layer 3 (500 neurons)\n",
    "5. Hidden layer 4 (500 neurons)\n",
    "6. Output layer\n",
    "\n",
    "Like the previous two models, we are going to define our input placeholders, weight, and bias matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/darth/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorFlow library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data reader\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Load the MNIST data\n",
    "mnist = input_data.read_data_sets('/home/darth/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the hyper-parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of neurons per hidden layer\n",
    "num_nodes_hl1 = 500\n",
    "num_nodes_hl2 = 500\n",
    "num_nodes_hl3 = 500\n",
    "num_nodes_hl4 = 500\n",
    "\n",
    "# number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# batch size\n",
    "batch_size = 100\n",
    "\n",
    "# number of passes through the input data\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, the placeholders\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "y_input = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the MLP architecture\n",
    "\n",
    "# define the hidden layers\n",
    "\n",
    "# (x_input * weights) + biases\n",
    "hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([784, num_nodes_hl1])),\n",
    "                 'biases': tf.Variable(tf.random_normal([num_nodes_hl1]))}\n",
    "\n",
    "# (hidden_layer_1 * weights) + biases\n",
    "hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([num_nodes_hl1, num_nodes_hl2])),\n",
    "                 'biases': tf.Variable(tf.random_normal([num_nodes_hl2]))}\n",
    "\n",
    "# (hidden_layer_2 * weights) + biases\n",
    "hidden_layer_3 = {'weights': tf.Variable(tf.random_normal([num_nodes_hl2, num_nodes_hl3])),\n",
    "                 'biases': tf.Variable(tf.random_normal([num_nodes_hl3]))}\n",
    "\n",
    "# (hidden_layer_3 * weights) + biases\n",
    "hidden_layer_4 = {'weights': tf.Variable(tf.random_normal([num_nodes_hl3, num_nodes_hl4])),\n",
    "                 'biases': tf.Variable(tf.random_normal([num_nodes_hl4]))}\n",
    "\n",
    "# (hidden_layer_4 * weights) + biases\n",
    "output_layer = {'weights': tf.Variable(tf.random_normal([num_nodes_hl4, num_classes])),\n",
    "                 'biases': tf.Variable(tf.random_normal([num_classes]))}\n",
    "\n",
    "# operations on hidden layers\n",
    "l1 = tf.add(tf.matmul(x_input, hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "l1 = tf.nn.relu(l1)\n",
    "\n",
    "l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "l2 = tf.nn.relu(l2)\n",
    "\n",
    "l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "l3 = tf.nn.relu(l3)\n",
    "\n",
    "l4 = tf.add(tf.matmul(l3, hidden_layer_4['weights']), hidden_layer_4['biases'])\n",
    "l4 = tf.nn.relu(l4)\n",
    "\n",
    "# the output to be fed to softmax\n",
    "output = tf.matmul(l4, output_layer['weights']) + output_layer['biases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final layer of the MLP, \"squash\" down the output values to probability distribution \\\\([0, 1] \\in \\mathbb{R}\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model's loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_input))\n",
    "\n",
    "# train step, with default learning_rate\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now begin to train our defined model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 10 loss : 239792.39849121094\n",
      "Epoch 2 completed out of 10 loss : 52766.10199901581\n",
      "Epoch 3 completed out of 10 loss : 27240.31031768799\n",
      "Epoch 4 completed out of 10 loss : 15701.058897218703\n",
      "Epoch 5 completed out of 10 loss : 9622.26944158554\n",
      "Epoch 6 completed out of 10 loss : 5459.887216365635\n",
      "Epoch 7 completed out of 10 loss : 4200.922947282791\n",
      "Epoch 8 completed out of 10 loss : 3762.9942823529245\n",
      "Epoch 9 completed out of 10 loss : 4194.870757398606\n",
      "Epoch 10 completed out of 10 loss : 3288.1974897241594\n",
      "Test Accuracy : 0.94921875\n"
     ]
    }
   ],
   "source": [
    "# variables initializer\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# start a TF session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _ in range(int(mnist.train.num_examples / batch_size)):\n",
    "            # get the data by batch\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # define dictionary for input \n",
    "            feed_dict = {x_input: batch_x, y_input: batch_y}\n",
    "            \n",
    "            # run the train_step and cross_entropy with the previously-defined inputs\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            \n",
    "            # record the loss\n",
    "            epoch_loss += loss\n",
    "            \n",
    "        # display training status\n",
    "        print('Epoch {} completed out of {} loss : {}'.format(epoch + 1, epochs, epoch_loss / batch_size))\n",
    "        \n",
    "    # get the accuracy of the trained model\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_input, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float16))\n",
    "    print('Test Accuracy : {}'.format(accuracy.eval({x_input: mnist.test.images, y_input: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of approximately 94.9% is not surprising since as mentioned a while ago, MLP can be likened to logistic regression. Difference being it has non-linearities. Thus, this relatively-lower (compared to previous session's CNN) accuracy is \"forgiveable\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
