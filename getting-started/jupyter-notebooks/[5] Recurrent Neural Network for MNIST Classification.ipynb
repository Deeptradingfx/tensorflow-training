{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "MLPs are used for a lot of problems, e.g. pattern recognition, classification, function approximation, etc. But they are not intrinsically intelligent as they lack the emulaton of a human memory's associative characteristics. For such purpose, we need a __recurrent neural network__ (RNN).\n",
    "\n",
    "![](../figures/rnn-unrolled.png)\n",
    "\n",
    "Image from [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.\n",
    "\n",
    "An RNN has a feedback loop from its outputs to inputs. This feedback loop enables the neural network to retain information (refer to image above). For instance, if it has the following input:\n",
    "\n",
    "    Michael C. was born in Paris, France. He is married, and has three children. He received a M.S. in neurosciences from the University Pierre & Marie Curie and the Ecole Normale Superieure in 1987, and ... His mother tongue is ???\n",
    "    \n",
    "If the neural network was tasked to predict what is the next value, i.e. the answer to the 'mother tongue' sequence, an RNN is the architecture that must be chosen. This storage of information is the duty of the feedback loop in an RNN. However, like the given example, there are a lot of unrelated information in between the two for the model to determine what is the answer. The context that gives the answer to the question is that Michael C. was born in Paris, France. But again, a lot of information before the mother tongue description comes into light.\n",
    "\n",
    "This problem is called the _long term dependency problem_. The RNN fails to remember that much of information as it progresses through time (input). The solution to this problem is the use of [Long Short Term Memory (LSTM)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (refer to the image below). In a nutshell, LSTM has the functionality to determine which input is necessary to the next time step, i.e. which information it must keep and which must not be kept.\n",
    "\n",
    "![](../figures/rnn-lstm.png)\n",
    "\n",
    "Image from [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.\n",
    "\n",
    "An improvement on the LSTM model is the Gated Recurrent Unit (GRU), which is computationally more efficient than the LSTM. For this session, we are going to implement GRU for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/darth/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/darth/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorFlow library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the NumPy library\n",
    "import numpy as np\n",
    "\n",
    "# Load the input reader\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Load the MNIST data\n",
    "mnist = input_data.read_data_sets('/home/darth/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyper-parameters\n",
    "BATCH_SIZE = 128\n",
    "CELL_SIZE = 32\n",
    "CHUNK_SIZE = 28\n",
    "HM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_CHUNKS = 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# input placeholders\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CHUNKS, CHUNK_SIZE])\n",
    "y_input = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CLASSES])\n",
    "initial_state = tf.placeholder(dtype=tf.float32, shape=[None, CELL_SIZE])\n",
    "\n",
    "# define the model\n",
    "cell = tf.contrib.rnn.GRUCell(CELL_SIZE)\n",
    "output, states = tf.nn.dynamic_rnn(cell, x_input, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# define the weights and biases\n",
    "xav_init = tf.contrib.layers.xavier_initializer\n",
    "weight = tf.get_variable('weights', shape=[CELL_SIZE, NUM_CLASSES], initializer=xav_init())\n",
    "bias = tf.get_variable('biases', initializer=tf.constant(0.1, shape=[NUM_CLASSES]))\n",
    "\n",
    "# the output of the RNN\n",
    "final_state = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(final_state, int(final_state.get_shape()[0]) - 1)\n",
    "\n",
    "# predicted value\n",
    "output = tf.matmul(last, weight) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the cost function and train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_input))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the measurement of the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_input, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `initial_state` of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this state will be updated as the network learns\n",
    "current_state = np.zeros([BATCH_SIZE, CELL_SIZE])\n",
    "\n",
    "# variables initializer\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 completed out of 10, loss : 0.12615236639976501 accuracy 0.9609375\n",
      "Epoch : 2 completed out of 10, loss : 0.07255779206752777 accuracy 0.9765625\n",
      "Epoch : 3 completed out of 10, loss : 0.10057511180639267 accuracy 0.9609375\n",
      "Epoch : 4 completed out of 10, loss : 0.02002027817070484 accuracy 1.0\n",
      "Epoch : 5 completed out of 10, loss : 0.1277269870042801 accuracy 0.96875\n",
      "Epoch : 6 completed out of 10, loss : 0.08655179291963577 accuracy 0.96875\n",
      "Epoch : 7 completed out of 10, loss : 0.14416192471981049 accuracy 0.9609375\n",
      "Epoch : 8 completed out of 10, loss : 0.04098448529839516 accuracy 0.9921875\n",
      "Epoch : 9 completed out of 10, loss : 0.0689292699098587 accuracy 0.984375\n",
      "Epoch : 10 completed out of 10, loss : 0.035995617508888245 accuracy 0.9921875\n",
      "Test Accuracy : 0.98046875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for epoch in range(HM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for _ in range(mnist.train.num_examples // BATCH_SIZE):\n",
    "            # load the input data by batch\n",
    "            batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "            \n",
    "            # resize the image data\n",
    "            batch_x = batch_x.reshape((BATCH_SIZE, NUM_CHUNKS, CHUNK_SIZE))\n",
    "            \n",
    "            # create input dictionary\n",
    "            feed_dict = {x_input: batch_x, y_input: batch_y, initial_state: current_state}\n",
    "            \n",
    "            _, epoch_loss, train_accuracy, next_state = sess.run([train_step, cross_entropy, accuracy, states], feed_dict=feed_dict)\n",
    "            \n",
    "            \n",
    "        # display the state of the model\n",
    "        print('Epoch : {} completed out of {}, loss : {} accuracy {}'.format(epoch + 1, HM_EPOCHS, epoch_loss, train_accuracy))\n",
    "        \n",
    "        # update the RNN state\n",
    "        current_state = next_state\n",
    "    \n",
    "    # load the test data\n",
    "    x_test = mnist.test.images.reshape((-1, NUM_CHUNKS, CHUNK_SIZE))\n",
    "    y_test = mnist.test.labels\n",
    "    \n",
    "    # the state must not be the trained stated\n",
    "    # the size of test data is 10000\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x_input: x_test, y_input: y_test, initial_state: np.zeros([10000, CELL_SIZE])})\n",
    "    \n",
    "    # Display test accuracy\n",
    "    print('Test Accuracy : {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just few iterations, the GRU (RNN) model was able to reach a 98.04% test accuracy on MNIST classification. Further training will improve this accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
